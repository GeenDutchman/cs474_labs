{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeenDutchman/cs474_labs/blob/master/DL_Lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FintZZPtbu6"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "### Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "### Deliverable:\n",
        "- Fill in the code for the RNN (using PyTorch's built-in GRU).\n",
        "- Fill in the training loop\n",
        "- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n",
        "- Implement your own GRU cell.\n",
        "- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n",
        "\n",
        "### Grading Standards:\n",
        "- 20% Implementation the RNN\n",
        "- 20% Implementation training loop\n",
        "- 20% Implementation of evaluation loop\n",
        "- 20% Implementation of your own GRU cell\n",
        "- 20% Training of your RNN on a domain of your choice\n",
        "\n",
        "### Tips:\n",
        "- Read through all the helper functions, run them, and make sure you understand what they are doing\n",
        "- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n",
        "- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n",
        "\n",
        "### Example Output:\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling \n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (Take note that you will not be implementing the encoder part of this tutorial.)\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7bdZWxvJrsx",
        "outputId": "133df66e-40fe-40f6-cf11-fc46121178c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        " \n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-09 04:08:28--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 52.5.213.57, 3.224.25.41, 54.205.50.50, ...\n",
            "Connecting to piazza.com (piazza.com)|52.5.213.57|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2020-10-09 04:08:29--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 54.230.85.211, 54.230.85.196, 54.230.85.168, ...\n",
            "Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|54.230.85.211|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-10-09 04:08:29 (19.8 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "file_len = 2579888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxBeKeNjJ0NQ",
        "outputId": "289d06d3-1f31-48dd-fba1-7aa7aa7d7a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " all the old names.' \n",
            "\n",
            "'It is also called kingsfoil,' said Aragorn; 'and maybe you know it by \n",
            "that name, for so the country-folk call it in these latter days.' 'Oh that!' \n",
            "said Ioreth. 'Well, if your \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On0_WitWJ99e",
        "outputId": "2d5734ed-a44b-41e5-be3c-b1415a572198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor\n",
        "\n",
        "abcDEF_tensor = char_tensor('abcDEF')\n",
        "print(abcDEF_tensor)\n",
        "\n",
        "def tensor_char(tensor):\n",
        "  result_str = ''\n",
        "  for l in range(tensor.size()[0]):\n",
        "    result_str += all_characters[tensor[l]]\n",
        "  return result_str\n",
        "\n",
        "abcDEF_string = tensor_char(abcDEF_tensor)\n",
        "print(abcDEF_string)\n",
        "# twice_tensor = abcDEF_tensor.append(abcDEF_tensor)\n",
        "# print(tensor_char(twice_tensor))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n",
            "abcDEF\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell \n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "* Create a custom GRU cell\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavAv50ZKQ-F"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    \n",
        "  \n",
        "  def forward(self, inputs, hidden):\n",
        "    # Each layer does the following:\n",
        "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "    \n",
        "    return outputs, hiddens\n",
        "  \n"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Create an RNN class that extends from nn.Module.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6tNdEnzWj5F"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1, soft_max=False):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    # more stuff here...\n",
        "    self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "    self.gru = nn.GRU(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.n_layers)\n",
        "    self.linOut = nn.Linear(self.hidden_size, self.output_size)\n",
        "    self.softmax = nn.Softmax(dim=1) if soft_max is True else None\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "    # of the GRU\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    # stuff here\n",
        "    out_decoded = self.embedding(input_char).view(1, 1, -1)\n",
        "    out_decoded = F.relu(out_decoded)\n",
        "    out_decoded, hidden = self.gru(out_decoded, hidden)\n",
        "    out_decoded = self.linOut(out_decoded[0])\n",
        "    if self.softmax is not None:\n",
        "      out_decoded = self.softmax(out_decoded)\n",
        "    \n",
        "    return out_decoded, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrhXghEPKD-5",
        "outputId": "f656aee7-1abe-4a6f-92d3-c3e1e4916b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target\n",
        "\n",
        "print(random_training_set())"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([28, 94, 21, 18, 20, 14, 94, 27, 10, 18, 23, 94, 24, 23, 94, 10, 94, 32,\n",
            "        18, 21, 29, 14, 13, 94, 21, 14, 29, 29, 30, 12, 14, 75, 94, 55, 17, 14,\n",
            "        27, 14, 62, 94, 49, 24, 32, 94, 44, 94, 12, 10, 23, 94, 20, 14, 14, 25,\n",
            "        94, 96, 10, 32, 10, 20, 14, 94, 21, 24, 23, 16, 94, 14, 23, 24, 30, 16,\n",
            "        17, 94, 29, 24, 94, 14, 10, 29, 94, 10, 94, 11, 18, 29, 75, 68, 94, 96,\n",
            "        96, 55, 17, 14, 34, 94, 32, 14, 27, 14, 94, 21, 14, 13, 94, 29, 17, 14,\n",
            "        23, 94, 29, 24, 94, 28, 14, 10, 29, 28, 94, 11, 14, 28, 18, 13, 14, 94,\n",
            "        41, 10, 27, 10, 22, 18, 27, 77, 94, 11, 10, 27, 27, 14, 21, 28, 94, 12,\n",
            "        24, 31, 14, 27, 14, 13, 94, 32, 18, 29, 17, 94, 25, 14, 21, 29, 28, 94,\n",
            "        96, 10, 23, 13, 94, 17, 18, 16, 17, 94, 14, 23, 24, 30, 16, 17, 94, 10,\n",
            "        11, 24, 31, 14, 94, 29, 17, 14, 94, 11, 14, 23, 12, 17, 14, 28, 94, 24,\n",
            "        15, 94]), tensor([94, 21, 18, 20, 14, 94, 27, 10, 18, 23, 94, 24, 23, 94, 10, 94, 32, 18,\n",
            "        21, 29, 14, 13, 94, 21, 14, 29, 29, 30, 12, 14, 75, 94, 55, 17, 14, 27,\n",
            "        14, 62, 94, 49, 24, 32, 94, 44, 94, 12, 10, 23, 94, 20, 14, 14, 25, 94,\n",
            "        96, 10, 32, 10, 20, 14, 94, 21, 24, 23, 16, 94, 14, 23, 24, 30, 16, 17,\n",
            "        94, 29, 24, 94, 14, 10, 29, 94, 10, 94, 11, 18, 29, 75, 68, 94, 96, 96,\n",
            "        55, 17, 14, 34, 94, 32, 14, 27, 14, 94, 21, 14, 13, 94, 29, 17, 14, 23,\n",
            "        94, 29, 24, 94, 28, 14, 10, 29, 28, 94, 11, 14, 28, 18, 13, 14, 94, 41,\n",
            "        10, 27, 10, 22, 18, 27, 77, 94, 11, 10, 27, 27, 14, 21, 28, 94, 12, 24,\n",
            "        31, 14, 27, 14, 13, 94, 32, 18, 29, 17, 94, 25, 14, 21, 29, 28, 94, 96,\n",
            "        10, 23, 13, 94, 17, 18, 16, 17, 94, 14, 23, 24, 30, 16, 17, 94, 10, 11,\n",
            "        24, 31, 14, 94, 29, 17, 14, 94, 11, 14, 23, 12, 17, 14, 28, 94, 24, 15,\n",
            "        94, 29]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes. \n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Fill in the pieces.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ALC3Pf8Kbsi"
      },
      "source": [
        "# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n",
        "def train(inp, target, per_char=True, teacher_forcing=True):\n",
        "  ## initialize hidden layers, set up gradient and loss \n",
        "    # your code here\n",
        "  ## /\n",
        "  inp_len = inp.size(0)\n",
        "  target_len = target.size(0)\n",
        "\n",
        "  decoder.train()\n",
        "  decoder_optimizer.zero_grad()\n",
        "  hidden = decoder.init_hidden()\n",
        "  loss = 0\n",
        "  \n",
        "  # result = []\n",
        "  # more stuff here...\n",
        "  if per_char:\n",
        "    next_char = inp[0]\n",
        "    for index in range(inp_len):\n",
        "      output, hidden = decoder(next_char, hidden)\n",
        "      values, indicies = output.topk(1)\n",
        "      # print(values, indicies)\n",
        "      result_char = indicies.squeeze().detach()\n",
        "      # result.append(result_char)\n",
        "      if teacher_forcing:\n",
        "        next_char = target[index]\n",
        "      else:\n",
        "        next_char = result_char\n",
        "      # output.transpose_(1, 2)\n",
        "      # print(output.size(), output.squeeze().size(), target[index].view(-1).size())\n",
        "      loss += criterion(output, target[index].view(-1))\n",
        "\n",
        "  else:\n",
        "    result, hidden = decoder(inp, hidden)\n",
        "    loss += criterion(result, target)\n",
        "  \n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "  \n",
        "  return loss.item() / target_len\n"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-bp-OZ1KjNh"
      },
      "source": [
        "def sample_outputs(output, temperature):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    # print(output.size())\n",
        "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "    # your code here\n",
        "  ## /\n",
        "  torch.no_grad()\n",
        "  decoder.eval()\n",
        "  result_str = ''\n",
        "  tensor_str = char_tensor(prime_str)\n",
        "\n",
        "  hidden = decoder.init_hidden()\n",
        "\n",
        "  predicted = []\n",
        "\n",
        "  next_char = 0 # declare it out of loop scope\n",
        "  for i in range(predict_len):\n",
        "    next_char = tensor_str[i] if i < tensor_str.size()[0] else next_char\n",
        "    # print(next_char, next_char < len(all_characters), len(all_characters))\n",
        "    output, hidden = decoder(next_char, hidden)\n",
        "    next_char = sample_outputs(output, temperature)\n",
        "    predicted.append(next_char)\n",
        "\n",
        "  predicted = torch.cat(predicted)\n",
        "  result_str = tensor_char(predicted)\n",
        "\n",
        "  return result_str\n",
        "\n",
        "\n",
        "# def scope(epoch):\n",
        "#   gc.collect()  "
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs gave.\n",
        "\n",
        "**TODO:** \n",
        "* Create some cool output\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nXFeCmdKodw"
      },
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKfozqw-6eqb",
        "outputId": "7e30d6b0-36fc-4840-c584-3a3e16e07b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "# n_epochs = 2000 \n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[123.56751537322998 (200 4%) 2.3801]\n",
            "W Gga conden wed an wexr Tlilny Bir Sot lai. \n",
            "mes bas, aus somgels bhe nleun. \n",
            "'\n",
            "bent mot carod wmpo \n",
            "\n",
            "[248.59755778312683 (400 8%) 2.1676]\n",
            "oe has to cut ine apor fare Eumink . cars ,cong of rorth am sad in store the nor hoth no \n",
            "the hirt i \n",
            "\n",
            "[372.1814432144165 (600 12%) 1.6855]\n",
            "einet Jore and a she come rievs ut clood though and so the mack was of they ladd \n",
            "in the your lode w \n",
            "\n",
            "[494.46493911743164 (800 16%) 1.5524]\n",
            "eeven was now the \n",
            "cpace manes all the asher.' \n",
            "\n",
            "'But lite am? \n",
            "\n",
            "'Waessed and beting was riepling,'  \n",
            "\n",
            "[618.414496421814 (1000 20%) 1.8164]\n",
            "han a can dam. It man wair have to their whed and Porn of with the grid, and the row, and at that mi \n",
            "\n",
            "[743.0812449455261 (1200 24%) 1.6310]\n",
            "houved, and \n",
            "foing to \n",
            "esanding was licking there wat little \n",
            "to some canted they can \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "[866.067773103714 (1400 28%) 1.8820]\n",
            "hing his neage. He could dound it urned. \n",
            "\n",
            "'There cook light him, and tree to the find kangern quick \n",
            "\n",
            "[992.3939967155457 (1600 32%) 1.3119]\n",
            "eat was are, and at for in a spreaked carning of do not we can were \n",
            "forcether the swind of the stre \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee0so6aKJ5L8"
      },
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    }
  ]
}